Supheria, [2023/2/11 18:40]
【乐正垂星】母函数是可以被理解的？！ https://b23.tv/fRSye2Q

Supheria, [2023/2/11 18:41]
啥

Supheria, [2023/2/11 18:41]
这个让我想到你之前分文件的需求

NnWinter, [2023/2/11 18:44]
好像是

NnWinter, [2023/2/11 18:44]
我看看

NnWinter, [2023/2/11 18:45]
我现在刚看到它能计算出有多少具有想通和的部分

NnWinter, [2023/2/11 18:45]
如果能具体确定求和是哪些，就可以大幅度优化算法了

Supheria, [2023/2/11 18:56]
他讲的还是烧脑，但我发现一个数学领域，组合数学

Supheria, [2023/2/11 18:56]
这个应该有用

NnWinter, [2023/2/11 18:57]
烧脑倒没有

NnWinter, [2023/2/11 18:57]
二进制学得好其实前半段完全理解

NnWinter, [2023/2/11 18:57]
不过看了以后发现用不上

NnWinter, [2023/2/11 18:57]
但提供了一个思路倒是不错

NnWinter, [2023/2/11 18:58]
就突然想到的

NnWinter, [2023/2/11 18:58]
举个例子，可以对文件进行一个偏差容错

NnWinter, [2023/2/11 18:59]
假设文件尺寸是按照之前我那个直方图分布的

那至少来说，文件的尺寸和最终要打的压缩包数量是相关的

NnWinter, [2023/2/11 18:59]
而文件的数量并不是很重要

NnWinter, [2023/2/11 19:59]
简单来说就是，既然目标是存储大的媒体文件，那么就可以设定一个阈值，定义一下什么样算小文件

比如你一个压缩包要打 2GB，那么 100MB 大概就算是小文件了

NnWinter, [2023/2/11 19:59]
要20个小文件，才能有 2GB 那么大，所以其实小文件的权重很低

NnWinter, [2023/2/11 20:00]
而大文件必然不可能很多

NnWinter, [2023/2/11 20:00]
比如你以 100 MB 作为一个阈值进行划分
假设你有 100GB 的文件要进行打包
假设每一个文件都超过 100MB
能有多少个文件？

NnWinter, [2023/2/11 20:02]
也不过才 1000 个文件对吧？
每个文件的大小，都是 long
那么问题来了
如果要把这些文件以我们之前假设的两种分布方式

1. 压缩包数量优先
2. 文件分布优先

那么如果穷举的话，就可以直接给出两种选择的最优解
并且可以让用户来自行决定使用哪一种

NnWinter, [2023/2/11 20:03]
而穷举的内容，也不过只是 1000 个 long
就算算法极其垃圾
迭代次数也就是 O(n^2) 或者 O(n^3) 咯

NnWinter, [2023/2/11 20:06]
假设对每个压缩包都允许有一定程度的缝隙，
不要求它正好塞满那种美观的话，
还可以对文件进行层级划分，
比如每100MB作为一个层级，
假设压缩包是 2GB 上限
那也才分了20层而已

NnWinter, [2023/2/11 20:06]
在排序的时候，就变成，每一组里含有多少个文件，至于具体多大，不太重要

NnWinter, [2023/2/11 20:07]
而100MB以下的小文件，就可以不管了，随便怎么塞也最多会多出来一个压缩包而已，不会出现多出两个分卷那种情况

NnWinter, [2023/2/11 20:09]
而且占比不高，比如你压缩 2GB 的压缩包
压100GB，这是50个压缩包，
假设大文件都被均匀放置了，只剩下小文件，
那要么应该会被分到第49个文件的后半部分
要么会分到第50个文件

NnWinter, [2023/2/11 20:23]
假设每个文件都留出了100MB的空隙
那49个文件就是 4.9GB 的空隙
由于剩下的都是小文件，不可能产生超过 100MB的空隙
所以整体结构会变成类似
1.9GB * 50 (有空隙的大文件) + 5GB (剩余的小文件)

通常来说，都会多出来一个压缩包 (目前我用是这样)
所以实际上用现在的算法也只是
2GB * 50 + 一个压缩包

所以实际上相当于
1.9GB * 51 + 3.1GB

只多了 3.1GB
分成 1.9 + 1.2

正常是 51 个压缩包
这样分就会变成 53 个压缩包
但这样分能让一个文件内包含许多小文件

尤其是对于常规的文件
你也看了那个直方图
能把小文件塞在一起
那可能解压一个 2GB 能解出来几千个文件

相比于把几十个小文件塞在大文件旁边，性价比高了太多
而压缩包数量只膨胀 4%
我觉得血赚

NnWinter, [2023/2/11 20:25]
这个压缩上限，压缩下限，偏向于减少压缩包数量 (上传优化)，还是偏向于文件堆积优化 (下载优化) 
都可以让用户来决定

在生成文件分布的 XML 之后，让用户自己看一下，是否要用这种结构来进行压缩，
对压缩包的数量，和文件分布是否满意
还可以生成类似于 windir 的那种图

NnWinter, [2023/2/11 20:29]
至于剩下的零散小文件，
就还是直接用 O(n) 的默认算法，
对已经按照尺寸排列好的文件从大到小塞

反正两个压缩包文件的分布数量差距也不会被大于下限值
无非就是 1.8, 1.9, 2.0 GB 的差别而已

NnWinter, [2023/2/11 20:38]
就还是像之前的极端情况来说

100 MB 的空间，有文件：

90 MB +  89 MB + 9MB + 2MB

被打包成
(90MB + 9MB) + (89MB) + (2MB)
而不是
(90MB) + (89MB + 9MB + 2MB)

的问题
不过这样的一个区块最多也就只差一个文件
而且相差的文件可能前后最多也就 50MB 以下

概率极低，而且稍微调一下参数就能解决

NnWinter, [2023/2/11 20:38]
至少不会出现那种，因为不分区块，出现大压缩包一下差出 1GB 那种离谱情况

NnWinter, [2023/2/11 20:40]
相比于现在的压缩来说，体积最多只变成了 106%
但小文件能基本保证放在一起了

NnWinter, [2023/2/11 20:40]
我觉得是个挺好的办法

NnWinter, [2023/2/11 20:41]
对算力要求也不高，因为小文件数量多是多，但不需要进行穷举
所以假设 100GB文件，无非是类似

O(900^2) + O(n) 而已

NnWinter, [2023/2/11 20:42]
就算有 1,000,000 个小文件，也就才 O(900^2) + O(1000^2)
更何况就算不考虑大文件，本身这些小文件的分布也是必算的

NnWinter, [2023/2/11 20:44]
下次有空就搞这个

NnWinter, [2023/2/11 20:44]
不过我先要把学校的课排完
然后再研究研究你那个

NnWinter, [2023/2/11 20:44]
这个不着急

NnWinter, [2023/2/11 20:45]
反正我已经发到GIT上了

Supheria, [2023/2/11 20:45]
平方是从哪来的

NnWinter, [2023/2/11 20:46]
100GB，900个100MB 的文件，和 1,000,000 个小文件

NnWinter, [2023/2/11 20:46]
对 900 个大文件进行穷举，估计优化之后就是 O(n^2) ~ O(n^3) 之间

NnWinter, [2023/2/11 20:47]
1,000,000 个小文件，O(n) = O(1000000) = O(1000^2)

NnWinter, [2023/2/11 20:49]
按照我的估计的话，可能较好的情况的时候，
是 O(2n·log2(n))
不好的时候大概是 O(2n^2)